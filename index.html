<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="description" content="MoDiT: A novel framework combining 3DMM with a Diffusion-based Transformer for audio-driven talking head generation.">
  <meta name="keywords" content="MoDiT, Talking Head Generation, Diffusion Transformer, 3DMM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MoDiT: Learning Highly Consistent 3D Motion Coefficients</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yucheng Wang<sup>1</sup>, Dan Xu<sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="mailto:ywangls@connect.ust.hk">ywangls@connect.ust.hk</a>,
              <a href="mailto:danxu@cse.ust.hk">danxu@cse.ust.hk</a>
            </span>
          </div>

          <div class="publication-links">
            <!-- arXiv Link -->
            <span class="link-block">
              <a href="https://arxiv.org/abs/xxxx.xxxxx"
                 class="external-link button is-normal is-rounded is-dark">
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <!-- Code Link -->
            <span class="link-block">
              <a href="https://github.com/dummy/modit"
                 class="external-link button is-normal is-rounded is-dark">
                </span>
                <span>Code</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/Teaser.png" alt="Teaser Image" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        MoDiT produces consistent and realistic talking head animations from single image driven by audio.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Audio-driven talking head generation is critical for applications such as virtual assistants, video games, and films, where natural lip movements are essential. Despite progress in this field, challenges remain in producing both consistent and realistic facial animations. Existing methods, often based on GANs or UNet-based diffusion models, face three major limitations:
          </p>
          <ul>
            <li>Temporal jittering caused by weak temporal constraints, resulting in frame inconsistencies.</li>
            <li>Identity drift due to insufficient 3D information extraction, leading to poor preservation of facial identity.</li>
            <li>Unnatural blinking behavior due to inadequate modeling of realistic blink dynamics.</li>
          </ul>
          <p>
            To address these issues, we propose MoDiT, a novel framework that combines the 3D Morphable Model (3DMM) with a Diffusion-based Transformer. Our contributions include:
          </p>
          <ol>
            <li>A hierarchical denoising strategy with revised temporal attention and biased self/cross-attention mechanisms, enabling the model to refine lip synchronization and progressively enhance full-face coherence, effectively mitigating temporal jittering.</li>
            <li>The integration of 3DMM coefficients to provide explicit spatial constraints, ensuring accurate 3D-informed optical flow prediction and improved lip synchronization using Wav2Lip results, thereby preserving identity consistency.</li>
            <li>A refined blinking strategy to model natural eye movements, with smoother and more realistic blinking behaviors.</li>
          </ol>
        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <img src="./static/images/pipeline_image.jpg" alt="Pipeline Image" style="width: 100%; height: auto;">
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-half">
        <img src="./static/images/table_image1.jpg" alt="Table 1 Image" style="width: 100%; height: auto;">
      </div>
      <div class="column is-half">
        <img src="./static/images/table_image2.jpg" alt="Table 2 Image" style="width: 100%; height: auto;">
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3">Demo Video</h2>
        <video controls autoplay muted loop playsinline style="width: 100%; height: auto;">
          <source src="./static/videos/demo_video.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{modit2025,
  author    = {Yucheng Wang and Dan Xu},
  title     = {MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation},
  journal   = {arXiv preprint arXiv:xxxx.xxxxx},
  year      = {2025},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Source code for this website is based on the <a href="https://nerfies.github.io/">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>